{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./datos/x_train.npy')\n",
    "x_test = np.load('./datos/x_test.npy')\n",
    "y_train = np.load('./datos/y_train.npy')\n",
    "y_c1_train = np.load('./datos/y_c1_train.npy')\n",
    "y_c2_train = np.load('./datos/y_c2_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de imágenes por clase en y_train: [500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n",
      " 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n",
      " 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n",
      " 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n",
      " 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n",
      " 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n",
      " 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n",
      " 500. 500.]\n",
      "Cantidad de imágenes por clase en y_c1_train: [ 5000.  7500.  7500.  5000. 12500.  5000.  2500.  5000.]\n",
      "Cantidad de imágenes por clase en y_c2_train: [2500. 2500. 2500. 2500. 2500. 2500. 2500. 2500. 2500. 2500. 2500. 2500.\n",
      " 2500. 2500. 2500. 2500. 2500. 2500. 2500. 2500.]\n"
     ]
    }
   ],
   "source": [
    "def count_classes(y):\n",
    "    return np.sum(y, axis=0)\n",
    "\n",
    "class_counts_y_train = count_classes(y_train)\n",
    "class_counts_y_c1_train = count_classes(y_c1_train)\n",
    "class_counts_y_c2_train = count_classes(y_c2_train)\n",
    "\n",
    "print(\"Cantidad de imágenes por clase en y_train:\", class_counts_y_train)\n",
    "print(\"Cantidad de imágenes por clase en y_c1_train:\", class_counts_y_c1_train)\n",
    "print(\"Cantidad de imágenes por clase en y_c2_train:\", class_counts_y_c2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count_c1 = sum(class_counts_y_c1_train)\n",
    "class_weights_c1 = [total_count_c1 / (len(class_counts_y_c1_train) * count) for count in class_counts_y_c1_train]\n",
    "class_weights_tensor_c1 = torch.FloatTensor(class_weights_c1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, y_c1_train, y_c1_val, y_c2_train, y_c2_val = train_test_split(\n",
    "    x_train, y_train, y_c1_train, y_c2_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, y_c1_data, y_c2_data):\n",
    "        self.x_data = torch.tensor(x_data, dtype=torch.float32).permute(0, 3, 1, 2) # Permutamos para el formato [batch, channels, height, width] que es el que ocupan las capas Conv2D\n",
    "        self.y_data = torch.tensor(y_data, dtype=torch.float32)\n",
    "        self.y_c1_data = torch.tensor(y_c1_data, dtype=torch.float32)\n",
    "        self.y_c2_data = torch.tensor(y_c2_data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Devuelve una muestra junto con sus tres etiquetas\n",
    "        return self.x_data[idx], self.y_data[idx], self.y_c1_data[idx], self.y_c2_data[idx]\n",
    "\n",
    "    def shape(self):\n",
    "        print('x_data shape:', self.x_data.shape, 'tipo dato:', self.x_data.dtype)\n",
    "        print('y_data shape:', self.y_data.shape, 'tipo dato:', self.y_data.dtype)\n",
    "        print('y_c1_data shape:', self.y_c1_data.shape, 'tipo dato:', self.y_c1_data.dtype)\n",
    "        print('y_c2_data shape:', self.y_c2_data.shape, 'tipo dato:', self.y_c2_data.dtype)\n",
    "\n",
    "train_dataset = HierarchicalDataset(X_train, y_train, y_c1_train, y_c2_train)\n",
    "val_dataset = HierarchicalDataset(X_val, y_val, y_c1_val, y_c2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(Net1, self).__init__()\n",
    "\n",
    "        # Bloque 1\n",
    "        self.conv1 = nn.Conv2d(3, 256, kernel_size=3, padding=1)\n",
    "        # output dimension: (256, 32, 32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(256, 164, kernel_size=3, padding=1)\n",
    "        # output dimension: (164, 32, 32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(164, 128, kernel_size=3, padding=1)\n",
    "        # output dimension: (128, 32, 32)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Capa densa para clasificación y_c1\n",
    "        # output dimension = (128, 16, 16)\n",
    "\n",
    "        # Capa densa para clasificación y_c1\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n",
    "        self.dropout_fc1 = nn.Dropout(dropout)\n",
    "        self.output1 = nn.Linear(256, 8)\n",
    "\n",
    "        # Bloque 2\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, padding=1)\n",
    "        # output dimension: (512, 16, 16)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        # output dimension: (256, 16, 16)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        # output dimension: (128, 16, 16)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        # output dimension = (128, 8, 8)\n",
    "\n",
    "        # Capa densa para clasificación y_c1\n",
    "        self.fc2 = nn.Linear(128 * 8 * 8, 256)  # Ajustar según el tamaño de entrada\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout_fc2 = nn.Dropout(dropout)\n",
    "        self.output2 = nn.Linear(256, 20)\n",
    "\n",
    "        # Bloque 3\n",
    "        self.conv7 = nn.Conv2d(128, 512, kernel_size=3, padding=1)\n",
    "        # output dimension: (512, 8, 8)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(512, 264, kernel_size=3, padding=1)\n",
    "        # output dimension: (264, 8, 8)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(264, 128, kernel_size=3, padding=1)\n",
    "        # output dimension: (128, 8, 8)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # Pooling para reducir dimensiones\n",
    "        # output dimension = (128, 4, 4)\n",
    "\n",
    "        # Capa densa para clasificación y\n",
    "        self.fc3 = nn.Linear(128 * 4 * 4, 256)  # Ajustar según el tamaño de entrada\n",
    "        self.relu_fc3 = nn.ReLU()\n",
    "        self.dropout_fc3 = nn.Dropout(dropout)\n",
    "        self.output3 = nn.Linear(256, 100)  # Salida para el tercer nivel de clasificación\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bloque 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(x)\n",
    "        x1 = x.view(x.size(0), -1)  # Aplanar para capa densa\n",
    "        x1 = self.fc1(x1)\n",
    "        x1 = self.dropout_fc1(x1)\n",
    "        y_c1 = self.output1(x1)\n",
    "\n",
    "        # Bloque 2\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.pool2(x)  # Max Pooling antes de la capa densa\n",
    "        x = self.dropout(x)\n",
    "        x2 = x.view(x.size(0), -1)\n",
    "        x2 = self.fc2(x2)\n",
    "        x2 = self.dropout_fc2(x2)\n",
    "        y_c2 = self.output2(x2)\n",
    "\n",
    "        # Bloque 3\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout(x)\n",
    "        x3 = x.view(x.size(0), -1)\n",
    "        x3 = self.fc3(x3)\n",
    "        x3 = self.dropout_fc3(x3)\n",
    "        y_c = self.output3(x3)\n",
    "\n",
    "        return y_c, y_c1, y_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(Net2, self).__init__()\n",
    "\n",
    "        # Bloque 1\n",
    "        self.conv1 = nn.Conv2d(3, 256, kernel_size=3, padding=1)\n",
    "        # output (256, 64, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        # output (256, 32, 32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(256, 164, kernel_size=3, padding=1)\n",
    "        # output (164, 32, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        # output (164, 16, 16)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(164, 128, kernel_size=3, padding=1)\n",
    "        # output (128, 16, 16)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(1, 1)\n",
    "        # output (128, 16, 16)\n",
    "\n",
    "        # Capa densa para clasificación y_c1\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout_fc1 = nn.Dropout(dropout)\n",
    "        self.output1 = nn.Linear(256, 8)\n",
    "\n",
    "        # Bloque 2\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, padding=1)\n",
    "        # output (512, 16, 16)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(1, 1)\n",
    "        # output (512, 16, 16)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        # output dimension (512, 16, 16)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d(1, 1)\n",
    "        # output dimension (512, 16, 16)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        # output dimension (128, 16, 16)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.pool6 = nn.MaxPool2d(1, 1)\n",
    "        # output dimension (128, 16, 16)\n",
    "\n",
    "        # Capa densa para clasificación y_c2\n",
    "        self.fc2 = nn.Linear(128 * 16 * 16, 256)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout_fc2 = nn.Dropout(dropout)\n",
    "        self.output2 = nn.Linear(256, 20)\n",
    "\n",
    "\n",
    "        # Bloque 3\n",
    "        self.conv7 = nn.Conv2d(128, 512, kernel_size=3, padding=1)  # Cambiar el número de filtros según sea necesario\n",
    "        # output dimension (512, 16, 16)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.pool7 = nn.MaxPool2d(2, 2)\n",
    "        # output dimension (512, 8, 8)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        # output dimension (256, 8, 8)\n",
    "        self.relu8 = nn.ReLU()\n",
    "        self.pool8 = nn.MaxPool2d(2, 2)\n",
    "        # output dimension (256, 4, 4)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        # output dimension (128, 4, 4)\n",
    "        self.relu9 = nn.ReLU()\n",
    "        self.pool9 = nn.MaxPool2d(2, 2)\n",
    "        # output dimension (128, 2, 2)\n",
    "\n",
    "        # Capa densa para clasificación y\n",
    "        self.fc3 = nn.Linear(128 * 2 * 2, 256)\n",
    "        self.relu_fc3 = nn.ReLU()\n",
    "        self.dropout_fc3 = nn.Dropout(dropout)\n",
    "        self.output3 = nn.Linear(256, 100)  # Salida para el tercer nivel de clasificación\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bloque 1\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x1 = x.view(x.size(0), -1)\n",
    "        x1 = self.dropout_fc1(self.relu_fc1(self.fc1(x1)))\n",
    "        print('Primera capa lista')\n",
    "        y_c1 = self.output1(x1)\n",
    "\n",
    "        # Bloque 2\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.relu5(self.conv5(x))\n",
    "        x = self.relu6(self.conv6(x))\n",
    "        x2 = x.view(x.size(0), -1)  # Aplanar para capa densa\n",
    "        x2 = self.dropout_fc2(self.relu_fc2(self.fc2(x2)))\n",
    "        print('Segunda capa lista')\n",
    "        y_c2 = self.output2(x2)\n",
    "\n",
    "        # Bloque 3\n",
    "        x = self.relu7(self.conv7(x))\n",
    "        x = self.relu8(self.conv8(x))\n",
    "        x = self.relu9(self.conv9(x))\n",
    "        x3 = x.view(x.size(0), -1)  # Aplanar para capa densa\n",
    "        x3 = self.dropout_fc3(self.relu_fc3(self.fc3(x3)))\n",
    "        print('Tercera capa lista')\n",
    "        y_c = self.output3(x3)\n",
    "\n",
    "        return y_c1, y_c2, y_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, optimizer, num_epochs, train_loader, val_loader, criterion_y, criterion_y_c1, criterion_y_c2, device):\n",
    "    # Mover el modelo al dispositivo\n",
    "    model.to(device)\n",
    "\n",
    "    # Inicializar listas para almacenar los valores de loss, accuracy, precision, recall y F1\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies_y, train_accuracies_y_c1, train_accuracies_y_c2, train_accuracies_total = [], [], [], []\n",
    "    val_accuracies_y, val_accuracies_y_c1, val_accuracies_y_c2, val_accuracies_total = [], [], [], []\n",
    "    val_precisions, val_recalls, val_f1_scores = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_y, correct_y_c1, correct_y_c2, correct_total = 0, 0, 0, 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs, labels_y, labels_y_c1, labels_y_c2 = batch  # Desempaquetar el batch\n",
    "\n",
    "            # Mover los datos al dispositivo\n",
    "            inputs, labels_y, labels_y_c1, labels_y_c2 = inputs.to(device), labels_y.to(device), labels_y_c1.to(device), labels_y_c2.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Limpiar los gradientes\n",
    "\n",
    "            # Hacer una pasada hacia adelante\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calcular las pérdidas\n",
    "            loss_y = criterion_y(outputs[0], labels_y)\n",
    "            loss_y_c1 = criterion_y_c1(outputs[1], labels_y_c1)\n",
    "            loss_y_c2 = criterion_y_c2(outputs[2], labels_y_c2)\n",
    "            loss = loss_y + loss_y_c1 + loss_y_c2\n",
    "            loss.backward()  # Retropropagación\n",
    "            optimizer.step()  # Actualizar los pesos\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Obtener las predicciones\n",
    "            _, predicted_y = torch.max(outputs[0].data, 1)\n",
    "            _, predicted_y_c1 = torch.max(outputs[1].data, 1)\n",
    "            _, predicted_y_c2 = torch.max(outputs[2].data, 1)\n",
    "\n",
    "            # Convertir labels a índices de clases\n",
    "            labels_y = torch.argmax(labels_y, dim=1).to(device)\n",
    "            labels_y_c1 = torch.argmax(labels_y_c1, dim=1).to(device)\n",
    "            labels_y_c2 = torch.argmax(labels_y_c2, dim=1).to(device)\n",
    "\n",
    "            total += labels_y.size(0)\n",
    "            correct_y += (predicted_y == labels_y).sum().item()\n",
    "            correct_y_c1 += (predicted_y_c1 == labels_y_c1).sum().item()\n",
    "            correct_y_c2 += (predicted_y_c2 == labels_y_c2).sum().item()\n",
    "            if (predicted_y == labels_y).all() and (predicted_y_c1 == labels_y_c1).all() and (predicted_y_c2 == labels_y_c2).all():\n",
    "                correct_total += 1\n",
    "\n",
    "\n",
    "        # Calcular y almacenar el loss y accuracy promedio para el entrenamiento\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_train_accuracy_y = 100 * correct_y / total\n",
    "        avg_train_accuracy_y_c1 = 100 * correct_y_c1 / total\n",
    "        avg_train_accuracy_y_c2 = 100 * correct_y_c2 / total\n",
    "        avg_train_accuracy_total = 100 * correct_total / total\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies_y.append(avg_train_accuracy_y)\n",
    "        train_accuracies_y_c1.append(avg_train_accuracy_y_c1)\n",
    "        train_accuracies_y_c2.append(avg_train_accuracy_y_c2)\n",
    "        train_accuracies_total.append(avg_train_accuracy_total)\n",
    "\n",
    "        # Evaluar el modelo en el conjunto de validación\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct_y, val_correct_y_c1, val_correct_y_c2, val_correct_total = 0, 0, 0, 0\n",
    "        val_total = 0\n",
    "        val_all_preds_y, val_all_labels_y = [], []\n",
    "        val_all_preds_y_c1, val_all_labels_y_c1 = [], []\n",
    "        val_all_preds_y_c2, val_all_labels_y_c2 = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                val_inputs, val_labels_y, val_labels_y_c1, val_labels_y_c2 = val_batch\n",
    "\n",
    "                # Mover los datos de validación al dispositivo\n",
    "                val_inputs, val_labels_y, val_labels_y_c1, val_labels_y_c2 = val_inputs.to(device), val_labels_y.to(device), val_labels_y_c1.to(device), val_labels_y_c2.to(device)\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "\n",
    "                # Calcular las pérdidas\n",
    "                val_loss_y = criterion_y(val_outputs[0], val_labels_y)\n",
    "                val_loss_y_c1 = criterion_y_c1(val_outputs[1], val_labels_y_c1)\n",
    "                val_loss_y_c2 = criterion_y_c2(val_outputs[2], val_labels_y_c2)\n",
    "                val_loss = val_loss_y + val_loss_y_c1 + val_loss_y_c2\n",
    "                val_running_loss += val_loss.item()\n",
    "\n",
    "                # Obtener las predicciones de validación\n",
    "                _, val_predicted_y = torch.max(val_outputs[0].data, 1)\n",
    "                _, val_predicted_y_c1 = torch.max(val_outputs[1].data, 1)\n",
    "                _, val_predicted_y_c2 = torch.max(val_outputs[2].data, 1)\n",
    "\n",
    "                # Convertir labels a índices de clases\n",
    "                val_labels_y = torch.argmax(val_labels_y, dim=1).to(device)\n",
    "                val_labels_y_c1 = torch.argmax(val_labels_y_c1, dim=1).to(device)\n",
    "                val_labels_y_c2 = torch.argmax(val_labels_y_c2, dim=1).to(device)\n",
    "\n",
    "                val_total += val_labels_y.size(0)\n",
    "                val_correct_y += (val_predicted_y == val_labels_y).sum().item()\n",
    "                val_correct_y_c1 += (val_predicted_y_c1 == val_labels_y_c1).sum().item()\n",
    "                val_correct_y_c2 += (val_predicted_y_c2 == val_labels_y_c2).sum().item()\n",
    "                if (val_predicted_y == val_labels_y).all() and (val_predicted_y_c1 == val_labels_y_c1).all() and (val_predicted_y_c2 == val_labels_y_c2).all():\n",
    "                    val_correct_total += 1\n",
    "\n",
    "                # Almacenar todas las predicciones y etiquetas de validación\n",
    "                val_all_preds_y.extend(val_predicted_y.cpu().numpy())\n",
    "                val_all_labels_y.extend(val_labels_y.cpu().numpy())\n",
    "                val_all_preds_y_c1.extend(val_predicted_y_c1.cpu().numpy())\n",
    "                val_all_labels_y_c1.extend(val_labels_y_c1.cpu().numpy())\n",
    "                val_all_preds_y_c2.extend(val_predicted_y_c2.cpu().numpy())\n",
    "                val_all_labels_y_c2.extend(val_labels_y_c2.cpu().numpy())\n",
    "\n",
    "        # Calcular y almacenar el loss y accuracy promedio para la validación\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        avg_val_accuracy_y = 100 * val_correct_y / val_total\n",
    "        avg_val_accuracy_y_c1 = 100 * val_correct_y_c1 / val_total\n",
    "        avg_val_accuracy_y_c2 = 100 * val_correct_y_c2 / val_total\n",
    "        avg_val_accuracy_total = 100 * val_correct_total / val_total\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies_y.append(avg_val_accuracy_y)\n",
    "        val_accuracies_y_c1.append(avg_val_accuracy_y_c1)\n",
    "        val_accuracies_y_c2.append(avg_val_accuracy_y_c2)\n",
    "        val_accuracies_total.append(avg_val_accuracy_total)\n",
    "\n",
    "        # Calcular precisión, recall, F1 y soporte para cada nivel\n",
    "        precision_y, recall_y, f1_y, _ = precision_recall_fscore_support(val_all_labels_y, val_all_preds_y, average='weighted')\n",
    "        precision_y_c1, recall_y_c1, f1_y_c1, _ = precision_recall_fscore_support(val_all_labels_y_c1, val_all_preds_y_c1, average='weighted')\n",
    "        precision_y_c2, recall_y_c2, f1_y_c2, _ = precision_recall_fscore_support(val_all_labels_y_c2, val_all_preds_y_c2, average='weighted')\n",
    "        val_precisions.append((precision_y, precision_y_c1, precision_y_c2))\n",
    "        val_recalls.append((recall_y, recall_y_c1, recall_y_c2))\n",
    "        val_f1_scores.append((f1_y, f1_y_c1, f1_y_c2))\n",
    "\n",
    "        # Imprimir resultados finales de la época\n",
    "        print(f'\\nEpoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}, Accuracies Y/C1/C2: {avg_train_accuracy_y:.2f}%, {avg_train_accuracy_y_c1:.2f}%, {avg_train_accuracy_y_c2:.2f}%, Total Accuracy: {avg_train_accuracy_total:.2f}'\n",
    "              f' | Validation Loss: {avg_val_loss:.4f}, Accuracies Y/C1/C2: {avg_val_accuracy_y:.2f}%, {avg_val_accuracy_y_c1:.2f}%, {avg_val_accuracy_y_c2:.2f}%, Total Accuracy: {avg_val_accuracy_total}\\n')\n",
    "\n",
    "    # Graficar los resultados\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Gráfico de pérdidas\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Loss vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Gráfico de accuracies de cada nivel\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies_y, label='Training Accuracy Y')\n",
    "    plt.plot(train_accuracies_y_c1, label='Training Accuracy Y_c1')\n",
    "    plt.plot(train_accuracies_y_c2, label='Training Accuracy Y_c2')\n",
    "    plt.plot(train_accuracies_total, label='Training Accuracy Total')\n",
    "    plt.plot(val_accuracies_y, label='Validation Accuracy Y')\n",
    "    plt.plot(val_accuracies_y_c1, label='Validation Accuracy Y_c1')\n",
    "    plt.plot(val_accuracies_y_c2, label='Validation Accuracy Y_c2')\n",
    "    plt.plot(val_accuracies_total, label='Validation Accuracy Total')\n",
    "    plt.title('Accuracy vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies_total, train_accuracies_y, train_accuracies_y_c1, train_accuracies_y_c2, val_accuracies_total, val_accuracies_y, val_accuracies_y_c1, val_accuracies_y_c2, val_precisions, val_recalls, val_f1_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "model1 = Net1(dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 164\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "criterion_y = nn.CrossEntropyLoss()\n",
    "criterion_y_c1 = nn.CrossEntropyLoss(weight=class_weights_tensor_c1)\n",
    "criterion_y_c2 = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joann\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\joann\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/5] Training Loss: 9.6770, Accuracies Y/C1/C2: 1.01%, 12.16%, 4.75%, Total Accuracy: 0.00 | Validation Loss: 9.6762, Accuracies Y/C1/C2: 0.97%, 12.32%, 4.94%, Total Accuracy: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joann\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\joann\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/5] Training Loss: 9.6764, Accuracies Y/C1/C2: 1.00%, 12.32%, 4.75%, Total Accuracy: 0.00 | Validation Loss: 9.6762, Accuracies Y/C1/C2: 0.97%, 12.32%, 4.94%, Total Accuracy: 0.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, val_losses, train_accuracies_total, train_accuracies_y, train_accuracies_y_c1, train_accuracies_y_c2, val_accuracies_total, val_accuracies_y, val_accuracies_y_c1, val_accuracies_y_c2, val_precisions, val_recalls, val_f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_y_c1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_y_c2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(model, optimizer, num_epochs, train_loader, val_loader, criterion_y, criterion_y_c1, criterion_y_c2, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m loss_y_c2 \u001b[38;5;241m=\u001b[39m criterion_y_c2(outputs[\u001b[38;5;241m2\u001b[39m], labels_y_c2)\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_y \u001b[38;5;241m+\u001b[39m loss_y_c1 \u001b[38;5;241m+\u001b[39m loss_y_c2\n\u001b[1;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Retropropagación\u001b[39;00m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Actualizar los pesos\u001b[39;00m\n\u001b[0;32m     36\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_accuracies_total, train_accuracies_y, train_accuracies_y_c1, train_accuracies_y_c2, val_accuracies_total, val_accuracies_y, val_accuracies_y_c1, val_accuracies_y_c2, val_precisions, val_recalls, val_f1_scores = training(model1, optimizer, num_epochs, train_loader, val_loader, criterion_y, criterion_y_c1, criterion_y_c2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalDatasetPredict(Dataset):\n",
    "    def __init__(self, x_data):\n",
    "        self.x_data = torch.tensor(x_data, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx]\n",
    "\n",
    "ids = list(range(len(x_test)))\n",
    "\n",
    "x_test_dataset = HierarchicalDatasetPredict(x_test)\n",
    "\n",
    "# Create a DataLoader for your dataset\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "test_loader = DataLoader(x_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model_1.eval()\n",
    "\n",
    "submit_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        outputs = model_1(data)  # Ensure 'data' is a tensor\n",
    "\n",
    "        y_pred = outputs[0]\n",
    "        y_c1_pred = outputs[1]\n",
    "        y_c2_pred = outputs[2]\n",
    "\n",
    "        y_pred = y_pred.cpu()\n",
    "        y_c1_pred = y_c1_pred.cpu()\n",
    "        y_c2_pred = y_c2_pred.cpu()\n",
    "\n",
    "        y_pred_class = y_pred.argmax(dim=1).numpy()\n",
    "        y_c1_pred_class = y_c1_pred.argmax(dim=1).numpy()\n",
    "        y_c2_pred_class = y_c2_pred.argmax(dim=1).numpy()\n",
    "\n",
    "        # Collect predictions for each batch\n",
    "        for j in range(len(y_pred_class)):\n",
    "            submit_data.append(f\"{i*batch_size + j}, {y_c1_pred_class[j]} {y_c2_pred_class[j]} {y_pred_class[j]}\")\n",
    "\n",
    "# Write the results to 'submit.csv'\n",
    "with open('submit.csv', 'w') as file:\n",
    "    file.write(\"ID,Prediction\\n\")\n",
    "    for line in submit_data:\n",
    "        file.write(line + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
