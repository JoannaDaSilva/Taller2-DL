{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "43jTTntmIEEG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TyJ0gsYJIIN2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ9Hc_0ty9PD"
      },
      "source": [
        "# Exploratory Data Analisis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9NWU0XFcHsmb"
      },
      "outputs": [],
      "source": [
        "# Cargar los datos\n",
        "x_train = np.load('/content/x_train.npy')\n",
        "x_test = np.load('/content/x_test.npy')\n",
        "y_train = np.load('/content/y_train.npy')\n",
        "y_c1_train = np.load('/content/y_c1_train.npy')\n",
        "y_c2_train = np.load('/content/y_c2_train.npy')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVw2-0xJ8Q3Z"
      },
      "source": [
        "# División conjunto de entrenamiento y validación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLPwyYr08c_f"
      },
      "source": [
        "A partir del conjunto de datos `x_train`, se realiza la división para crear los conjuntos de entrenamiento y validación, respectivamente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5bAu9DLy3Ujl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zToldaGG83aV"
      },
      "source": [
        "Los parámetros `random_state` y `stratify` en la función `train_test_split` son fundamentales para asegurar una división adecuada de los datos. El parámetro `random_state` establece una semilla para el generador de números aleatorios, lo que garantiza que la división sea reproducible en ejecuciones futuras. Por otro lado, `stratify=y_train` asegura que la proporción de las clases en el conjunto de entrenamiento y el conjunto de validación sea la misma que en el conjunto original, lo que es especialmente importante en problemas de clasificación para mantener la representación adecuada de cada clase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "L5wP20en48P_"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val, y_c1_train, y_c1_val, y_c2_train, y_c2_val = train_test_split(\n",
        "    x_train, y_train, y_c1_train, y_c2_train,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLd_KFHv2Tuz"
      },
      "source": [
        "# Creación de Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK1uvKZu2i1V"
      },
      "source": [
        "Se define una clase personalizada `HierarchicalDataset` que extiende la clase `Dataset`. Esta clase se encargará de convertir los datos en tensores. Posteriormente, esta implementación nos permitirá crear los `DataLoaders` necesarios para la manipulación eficiente de los datos durante el entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IeSU1-4d8Y0m"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "a-xKCU-k2YC1"
      },
      "outputs": [],
      "source": [
        "class HierarchicalDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data, y_c1_data, y_c2_data, transform=None):\n",
        "        # Aplicar transformaciones si están definidas\n",
        "        self.x_data = torch.tensor(x_data, dtype=torch.float32).permute(0, 3, 1, 2)  # [N, C, H, W]\n",
        "        self.y_data = torch.tensor(y_data, dtype=torch.float32)\n",
        "        self.y_c1_data = torch.tensor(y_c1_data, dtype=torch.float32)\n",
        "        self.y_c2_data = torch.tensor(y_c2_data, dtype=torch.float32)\n",
        "        self.transform = transform  # Asegúrate de que esta línea esté presente\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Obtener la imagen y las etiquetas\n",
        "        sample = self.x_data[idx]\n",
        "        y = self.y_data[idx]\n",
        "        y_c1 = self.y_c1_data[idx]\n",
        "        y_c2 = self.y_c2_data[idx]\n",
        "\n",
        "        # Aplicar transformaciones si existen\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, y, y_c1, y_c2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "PH3ArCtW6VwD"
      },
      "outputs": [],
      "source": [
        "# Definir las transformaciones\n",
        "transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización típica\n",
        "])\n",
        "\n",
        "# Crear los datasets\n",
        "train_dataset = HierarchicalDataset(X_train, y_train, y_c1_train, y_c2_train, transform=transform)\n",
        "val_dataset = HierarchicalDataset(X_val, y_val, y_c1_val, y_c2_val, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY1XFW-e9Xto",
        "outputId": "6028547b-8ea4-42d7-fae9-1e88b4a283a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del Dataset de entrenamiento 40000\n",
            "Tamaño del Dataset de validación 10000\n"
          ]
        }
      ],
      "source": [
        "print('Tamaño del Dataset de entrenamiento', len(train_dataset))\n",
        "print('Tamaño del Dataset de validación', len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN_cNfzW6Lmj"
      },
      "source": [
        "# Creación DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUMJ7SZ66rAY"
      },
      "source": [
        "Se utilizan los datasets creados anteriormente para generar los `DataLoaders`, que facilitan la manipulación de los datos en lotes (batches) durante el entrenamiento.\n",
        "\n",
        "Antes de crear un `DataLoader`, es necesario definir el tamaño de los lotes (`batch_size`), que es un parámetro esencial para su creación.\n",
        "\n",
        "El parámetro `shuffle`, cuando se establece en `True`, permite que los datos sean barajados antes de ser divididos en los lotes. Esto garantiza que el orden de las muestras cambie en cada época durante el entrenamiento, lo que ayuda a reducir el sesgo. Cabe destacar que `shuffle=True` se utiliza únicamente durante el entrenamiento y no en la fase de validación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "88w_udOZ6Z1E"
      },
      "outputs": [],
      "source": [
        "batch_size = 164\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he_N4OnM9EsH",
        "outputId": "6d251b42-54ec-48c6-b68d-0a3b077dc493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño DataLoader de entrenamiento: 1250\n",
            "Tamaño DataLoader de validación: 313\n"
          ]
        }
      ],
      "source": [
        "print('Tamaño DataLoader de entrenamiento:', len(train_loader))\n",
        "print('Tamaño DataLoader de validación:', len(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXRbFGxO9ScP"
      },
      "source": [
        "Observamos que los tamaños de los `DataLoaders` no coinciden con los tamaños de los `Datasets` creados anteriormente. Esto se debe a la división en lotes que se realiza al crear los `DataLoaders`. Sabemos que el tamaño del conjunto de datos de entrenamiento es de 40,000 y el del conjunto de validación es de 10,000. Por lo tanto, el tamaño del `DataLoader` se calcula **dividiendo el tamaño del conjunto de datos entre el** `batch_size`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWW9qdsgyhxa"
      },
      "source": [
        "# Transfer Learning y Fine Tuning\n",
        "\n",
        "***Transfer learning*** es una técnica en deep learning que permite reutilizar un modelo preentrenado del estado del arte en una tarea similar. En lugar de entrenar un modelo desde cero, se aprovechan las características ya aprendidas por el modelo en conjuntos de datos grandes (como *ImageNet*, *Open Images*, *COCO*, y *Places365*), que contienen millones de imágenes. En este proceso, típicamente se reemplazan y ajustan solo las últimas capas densas para adaptarlas a la nueva tarea específica, acelerando el entrenamiento y mejorando la precisión en conjuntos de datos más pequeños.\n",
        "\n",
        "***Fine tuning***, por otro lado, es una técnica específica dentro del transfer learning que permite ajustar o \"afinar\" algunas o todas las capas del modelo preentrenado. Esto implica congelar las primeras capas, que han aprendido características generales, y dejar entrenables las capas superiores para adaptar el modelo a una tarea más específica.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acKZTMp2QvWJ"
      },
      "source": [
        "# Transfer learning con AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5Kzx5MA1hEQ"
      },
      "source": [
        "## Modelo AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cCByd48k-hPZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "fsF-HW2iRM_Z"
      },
      "outputs": [],
      "source": [
        "alexnet_model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5fzNwnC1UCc"
      },
      "source": [
        "Necesitamos mantener los parámetros ya aprendidos por el modelo intactos, por lo cuál se **congelan** los gradientes mediante  `requires_grad = False` en cada parámetro del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "JUuq3hkWT5FZ"
      },
      "outputs": [],
      "source": [
        "# Función para congelar las capas del modelo base\n",
        "def freeze_model(base_model):\n",
        "    for param in base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    return base_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "VesKU-jUUFMn"
      },
      "outputs": [],
      "source": [
        "alexnet_model = freeze_model(alexnet_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY1S-A9b5EKn"
      },
      "source": [
        "## Nuevo modelo con base AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bB5yodc5T5G"
      },
      "source": [
        "Definimos un nuevo modelo basado en el modelo de **AlexNet**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16gV9dIF3-3z"
      },
      "source": [
        "Necesitamos conocer la cantidad de `in_features` de la última capa de **AlexNet** para que la nueva capa densa que añadiremos se conecte correctamente en términos de dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "dS0kj_CrEmDG"
      },
      "outputs": [],
      "source": [
        "# Obtener el número de entradas de la última capa\n",
        "in_features = alexnet_model.classifier[6].in_features\n",
        "\n",
        "# Crear un nuevo clasificador\n",
        "num_classes_y = 100  # Cambia esto según tu problema\n",
        "num_classes_y_c1 = 8\n",
        "num_classes_y_c2 = 20\n",
        "\n",
        "# Reemplazar la última capa de clasificación y añadir nuevas capas\n",
        "alexnet_model.classifier[6] = nn.Linear(in_features, num_classes_y)\n",
        "alexnet_model.classifier.add_module('7', nn.Linear(in_features, num_classes_y_c1))\n",
        "alexnet_model.classifier.add_module('8', nn.Linear(in_features, num_classes_y_c2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJpfEwTc4iQp"
      },
      "source": [
        "La línea anterior accede a la segunda capa en el módulo `classifier` de EfficientNet. Esta capa es una capa `Linear` (totalmente conectada) que constituye la última capa del modelo original y define el número de entradas `in_features` y salidas `out_features` de la clasificación original en EfficientNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "uD2SUF0d3169"
      },
      "outputs": [],
      "source": [
        "class CustomAlexNet(nn.Module):\n",
        "    def __init__(self, num_fine_classes, num_coarse1_classes, num_coarse2_classes):\n",
        "        super(CustomAlexNet, self).__init__()\n",
        "\n",
        "        # Capas de características\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Capas de clasificación\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, num_fine_classes + num_coarse1_classes + num_coarse2_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        # Dividir la salida en tres partes\n",
        "        fine_output = x[:, :num_fine_classes]\n",
        "        coarse1_output = x[:, num_fine_classes:num_fine_classes + num_coarse1_classes]\n",
        "        coarse2_output = x[:, num_fine_classes + num_coarse1_classes:]\n",
        "\n",
        "        return fine_output, coarse1_output, coarse2_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trhwSgr5-Ow3",
        "outputId": "004c1971-cafe-4118-d7e6-967949c233b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CustomAlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
            "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Dropout(p=0.5, inplace=False)\n",
            "    (5): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "    (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Dropout(p=0.5, inplace=False)\n",
            "    (9): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Inicializar el modelo con las cantidades de clases\n",
        "num_fine_classes = 100\n",
        "num_coarse1_classes = 8\n",
        "num_coarse2_classes = 20\n",
        "\n",
        "model_base_alexnet = CustomAlexNet(num_fine_classes, num_coarse1_classes, num_coarse2_classes)\n",
        "\n",
        "print(model_base_alexnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv1sTZgpC2Wp"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Nt4rUN1p9si0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIQBX-e_68RZ"
      },
      "source": [
        "## Funciones para el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH6o-EN_ljnm",
        "outputId": "3574001f-9008-4f89-906f-912969b75690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 3, 32, 32]) torch.float32\n",
            "torch.Size([32, 100]) torch.float32\n",
            "torch.Size([32, 8]) torch.float32\n",
            "torch.Size([32, 20]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    inputs, labels_y, labels_y_c1, labels_y_c2 = batch\n",
        "    print(inputs.shape, inputs.dtype)\n",
        "    print(labels_y.shape, labels_y.dtype)\n",
        "    print(labels_y_c1.shape, labels_y_c1.dtype)\n",
        "    print(labels_y_c2.shape, labels_y_c2.dtype)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "LaIHzmpT22-0"
      },
      "outputs": [],
      "source": [
        "# Función de entrenamiento\n",
        "def training(model, optimizer, num_epochs, train_loader, val_loader, criterion_y, criterion_y_c1, criterion_y_c2, device):\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies_y, train_accuracies_y_c1, train_accuracies_y_c2 = [], [], []\n",
        "    val_accuracies_y, val_accuracies_y_c1, val_accuracies_y_c2 = [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Modo de entrenamiento\n",
        "        running_loss = 0.0\n",
        "        correct_y, correct_y_c1, correct_y_c2 = 0, 0, 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            inputs, labels_y, labels_y_c1, labels_y_c2 = batch\n",
        "            inputs, labels_y, labels_y_c1, labels_y_c2 = inputs.to(device), labels_y.to(device), labels_y_c1.to(device), labels_y_c2.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calcular pérdidas\n",
        "            loss_y = criterion_y(outputs[0], labels_y)\n",
        "            loss_y_c1 = criterion_y_c1(outputs[1], labels_y_c1)\n",
        "            loss_y_c2 = criterion_y_c2(outputs[2], labels_y_c2)\n",
        "            loss = loss_y + loss_y_c1 + loss_y_c2\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Obtener las predicciones\n",
        "            _, predicted_y = torch.max(outputs[0].data, 1)\n",
        "            _, predicted_y_c1 = torch.max(outputs[1].data, 1)\n",
        "            _, predicted_y_c2 = torch.max(outputs[2].data, 1)\n",
        "\n",
        "            # Convertir etiquetas one-hot a índices de clase\n",
        "            labels_y_idx = torch.argmax(labels_y, dim=1)\n",
        "            labels_y_c1_idx = torch.argmax(labels_y_c1, dim=1)\n",
        "            labels_y_c2_idx = torch.argmax(labels_y_c2, dim=1)\n",
        "\n",
        "            total += labels_y.size(0)\n",
        "            correct_y += (predicted_y == labels_y_idx).sum().item()\n",
        "            correct_y_c1 += (predicted_y_c1 == labels_y_c1_idx).sum().item()\n",
        "            correct_y_c2 += (predicted_y_c2 == labels_y_c2_idx).sum().item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_train_accuracy_y = 100 * correct_y / total\n",
        "        avg_train_accuracy_y_c1 = 100 * correct_y_c1 / total\n",
        "        avg_train_accuracy_y_c2 = 100 * correct_y_c2 / total\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accuracies_y.append(avg_train_accuracy_y)\n",
        "        train_accuracies_y_c1.append(avg_train_accuracy_y_c1)\n",
        "        train_accuracies_y_c2.append(avg_train_accuracy_y_c2)\n",
        "\n",
        "        # Evaluar el modelo en el conjunto de validación\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct_y, val_correct_y_c1, val_correct_y_c2 = 0, 0, 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_batch in val_loader:\n",
        "                val_inputs, val_labels_y, val_labels_y_c1, val_labels_y_c2 = val_batch\n",
        "                val_inputs, val_labels_y, val_labels_y_c1, val_labels_y_c2 = val_inputs.to(device), val_labels_y.to(device), val_labels_y_c1.to(device), val_labels_y_c2.to(device)\n",
        "\n",
        "                val_outputs = model(val_inputs)\n",
        "                val_loss_y = criterion_y(val_outputs[0], val_labels_y)\n",
        "                val_loss_y_c1 = criterion_y_c1(val_outputs[1], val_labels_y_c1)\n",
        "                val_loss_y_c2 = criterion_y_c2(val_outputs[2], val_labels_y_c2)\n",
        "                val_loss = val_loss_y + val_loss_y_c1 + val_loss_y_c2\n",
        "                val_running_loss += val_loss.item()\n",
        "\n",
        "                # Obtener predicciones\n",
        "                _, val_predicted_y = torch.max(val_outputs[0], 1)\n",
        "                _, val_predicted_y_c1 = torch.max(val_outputs[1], 1)\n",
        "                _, val_predicted_y_c2 = torch.max(val_outputs[2], 1)\n",
        "\n",
        "                val_labels_y_idx = torch.argmax(val_labels_y, dim=1)\n",
        "                val_labels_y_c1_idx = torch.argmax(val_labels_y_c1, dim=1)\n",
        "                val_labels_y_c2_idx = torch.argmax(val_labels_y_c2, dim=1)\n",
        "\n",
        "                val_total += val_labels_y.size(0)\n",
        "                val_correct_y += (val_predicted_y == val_labels_y_idx).sum().item()\n",
        "                val_correct_y_c1 += (val_predicted_y_c1 == val_labels_y_c1_idx).sum().item()\n",
        "                val_correct_y_c2 += (val_predicted_y_c2 == val_labels_y_c2_idx).sum().item()\n",
        "\n",
        "        avg_val_loss = val_running_loss / len(val_loader)\n",
        "        avg_val_accuracy_y = 100 * val_correct_y / val_total\n",
        "        avg_val_accuracy_y_c1 = 100 * val_correct_y_c1 / val_total\n",
        "        avg_val_accuracy_y_c2 = 100 * val_correct_y_c2 / val_total\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies_y.append(avg_val_accuracy_y)\n",
        "        val_accuracies_y_c1.append(avg_val_accuracy_y_c1)\n",
        "        val_accuracies_y_c2.append(avg_val_accuracy_y_c2)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}, '\n",
        "              f'Accuracies Y/C1/C2: {avg_train_accuracy_y:.2f}%, {avg_train_accuracy_y_c1:.2f}%, {avg_train_accuracy_y_c2:.2f}% '\n",
        "              f'| Validation Loss: {avg_val_loss:.4f}, '\n",
        "              f'Accuracies Y/C1/C2: {avg_val_accuracy_y:.2f}%, {avg_val_accuracy_y_c1:.2f}%, {avg_val_accuracy_y_c2:.2f}%')\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies_y, train_accuracies_y_c1, train_accuracies_y_c2, val_accuracies_y, val_accuracies_y_c1, val_accuracies_y_c2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "DUR0UkYq98-e"
      },
      "outputs": [],
      "source": [
        "# Configuraciones para el entrenamiento\n",
        "learning_rate = 0.00001\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "gti2CmzBSZdT"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion_y = nn.CrossEntropyLoss()\n",
        "criterion_y_c1 = nn.CrossEntropyLoss(weight=torch.tensor([1.0]*num_classes_y_c1)).to(device)\n",
        "criterion_y_c2 = nn.CrossEntropyLoss(weight=torch.tensor([1.0]*num_classes_y_c2)).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_base_alexnet.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeXdMn__-XaR",
        "outputId": "3672c691-b687-493a-f196-669bb1933ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50] Training Loss: 4.7945, Accuracies Y/C1/C2: 35.60%, 72.19%, 55.29% | Validation Loss: 5.0501, Accuracies Y/C1/C2: 35.61%, 68.20%, 52.21%\n"
          ]
        }
      ],
      "source": [
        "# Entrenar el modelo\n",
        "train_losses, val_losses, train_accuracies_y, train_accuracies_y_c1, train_accuracies_y_c2, val_accuracies_y, val_accuracies_y_c1, val_accuracies_y_c2 = training(\n",
        "    model_base_alexnet, optimizer, num_epochs, train_loader, val_loader, criterion_y, criterion_y_c1, criterion_y_c2, device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3ggC4uMTItM"
      },
      "outputs": [],
      "source": [
        "test_dataset = HierarchicalDataset(x_test, y_train, y_c1_train, y_c2_train, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY6rvDqNdfZg"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo en modo evaluación\n",
        "model_base_alexnet.eval()\n",
        "\n",
        "import torch\n",
        "\n",
        "all_predictions_y = []\n",
        "all_predictions_c1 = []\n",
        "all_predictions_c2 = []\n",
        "all_labels_y = []\n",
        "all_labels_c1 = []\n",
        "all_labels_c2 = []\n",
        "\n",
        "# Desactivar el cálculo de gradientes para la evaluación\n",
        "with torch.no_grad():\n",
        "    for images, labels_y, labels_c1, labels_c2 in test_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Hacer la predicción\n",
        "        outputs = model_base_alexnet(images)\n",
        "\n",
        "        # Obtener las predicciones para las etiquetas Y, coarse_1 y coarse_2\n",
        "        _, predicted_y = torch.max(outputs[0], 1)\n",
        "        _, predicted_c1 = torch.max(outputs[1], 1)\n",
        "        _, predicted_c2 = torch.max(outputs[2], 1)\n",
        "\n",
        "        # Guardar las predicciones\n",
        "        all_predictions_y.append(predicted_y.cpu())\n",
        "        all_predictions_c1.append(predicted_c1.cpu())\n",
        "        all_predictions_c2.append(predicted_c2.cpu())\n",
        "\n",
        "        # Guardar las etiquetas verdaderas\n",
        "        all_labels_y.append(labels_y.cpu())\n",
        "        all_labels_c1.append(labels_c1.cpu())\n",
        "        all_labels_c2.append(labels_c2.cpu())\n",
        "\n",
        "# Concatenar todas las predicciones y etiquetas\n",
        "all_predictions_y = torch.cat(all_predictions_y)\n",
        "all_predictions_c1 = torch.cat(all_predictions_c1)\n",
        "all_predictions_c2 = torch.cat(all_predictions_c2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Harka8gfdgGD",
        "outputId": "17d80127-a12f-4185-a304-685282f4a0b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_labels_y[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ff3gn98nuHX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Crear una lista con las predicciones \"coarse_1 coarse_2 fine_label\"\n",
        "predictions_list = [\n",
        "    f\"{c1.item()} {c2.item()} {y.item()}\"\n",
        "    for c1, c2, y in zip(all_predictions_c1, all_predictions_c2, all_predictions_y)\n",
        "]\n",
        "\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": range(len(predictions_list)),\n",
        "    \"Prediction\": predictions_list\n",
        "})\n",
        "\n",
        "\n",
        "submission_df.to_csv(\"submit.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
